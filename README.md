# üèì Table Tennis Match Prediction

**Competition**: [Introduction to Data Science Competition-TTMATCH](https://www.kaggle.com/competitions/introduction-to-data-secience-ttmatch)

**Final Score**: Private Leaderboard Score 0.3596

**AI Assistance**: This project was developed with the assistance of AI tools (Gemini, ChatGPT, Claude) for code development and documentation.

---

## üìä Competition Results

| Version | Private | Public | Delta | Key Features |
|---------|---------|--------|-------|-------------|
| **V6** (Competition) | 0.3419 | 0.3698 | -0.0279 ‚ùå | Rally context + 80-20 ensemble (overfitted) |
| **Gold** (Baseline) | 0.3574 | 0.3205 | +0.0369 ‚úÖ | Clean features + 50-50 ensemble (+15.5 pts) |
| **Gold + EDA** (Final) | **0.3596** üèÜ | 0.3189 | +0.0407 ‚úÖ | Distribution-aware features (+17.7 pts) |

### üéØ Key Insights

**What Went Wrong in V6**:
- ‚ùå Information leakage: `rally_serve_action/point` leaked future info
- ‚ùå Overfitting features: `is_deuce`, `is_server` too specific to train set
- ‚ùå Unbalanced ensemble: 80-20 blend reduced stability

**What Worked in Gold + EDA**:
- ‚úÖ Clean temporal features without leakage
- ‚úÖ Balanced 50-50 ensemble for robustness
- ‚úÖ Distribution-aware: Adapted to test set patterns (32.7% serves vs 15.9% in train)
- ‚úÖ Trust CV over public leaderboard

---

## üìÅ Project Structure

```
ttmatch/
‚îú‚îÄ‚îÄ README.md                      # This file
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ main_v6.ipynb             # ‚ùå Competition submission (overfitted)
‚îÇ   ‚îú‚îÄ‚îÄ main_gold.ipynb           # ‚úÖ Post-competition best baseline
‚îÇ   ‚îî‚îÄ‚îÄ main_gold_eda.ipynb       # üèÜ Final best model with EDA
‚îú‚îÄ‚îÄ analysis/
‚îÇ   ‚îî‚îÄ‚îÄ project_summary.txt       # Project documentation
‚îî‚îÄ‚îÄ requirements.txt               # Python dependencies

# Not included in repository (create locally):
‚îú‚îÄ‚îÄ data/                          # Download from Kaggle
‚îÇ   ‚îú‚îÄ‚îÄ train.csv
‚îÇ   ‚îú‚îÄ‚îÄ test.csv
‚îÇ   ‚îî‚îÄ‚îÄ sample_submission.csv
‚îî‚îÄ‚îÄ submissions/                   # Generated by notebooks
    ‚îú‚îÄ‚îÄ submission_v6.csv
    ‚îú‚îÄ‚îÄ submission_gold.csv
    ‚îî‚îÄ‚îÄ submission_gold_eda.csv
```

---

## üöÄ Quick Start

### 1. Environment Setup

```bash
# Clone repository (or download the project files)
cd ttmatch

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Data Preparation

Download competition data from Kaggle and create `data/` directory:

1. **Download Dataset**: Visit [Kaggle Competition Page](https://www.kaggle.com/competitions/introduction-to-data-secience-ttmatch)
2. **Create Structure**:
```bash
mkdir data
# Place downloaded files in data/ directory
```

Required files:
```
data/
‚îú‚îÄ‚îÄ train.csv                  # Training data
‚îú‚îÄ‚îÄ test.csv                   # Test data
‚îî‚îÄ‚îÄ sample_submission.csv      # Submission template
```

### 3. Run Models

#### Option A: Run Final Best Model (Gold + EDA) üèÜ
```bash
jupyter notebook notebooks/main_gold_eda.ipynb
# Execute all cells ‚Üí generates submission_gold_eda.csv
```

#### Option B: Run Baseline Model (Gold)
```bash
jupyter notebook notebooks/main_gold.ipynb
# Execute all cells ‚Üí generates submission_gold.csv
```

#### Option C: Reproduce Competition Submission (V6)
```bash
jupyter notebook notebooks/main_v6.ipynb
# Execute all cells ‚Üí generates submission_v6.csv
```

### 4. Expected Outputs

Each notebook generates predictions in `submissions/` directory:
- `submission_v6.csv`: V6 version predictions
- `submission_gold.csv`: Gold baseline predictions
- `submission_gold_eda.csv`: Final best predictions

**Note**: The `submissions/` directory will be created automatically when you run the notebooks.

Execution time: ~10-20 minutes (CPU), ~5-10 minutes (GPU)

---

## üî¨ Methodology

### Problem Definition

**Task**: Predict three targets for table tennis rally sequences:
1. **Next Action ID**: What stroke type will be played next? (20 classes)
2. **Next Point ID**: Where will the ball land? (11 classes)
3. **Rally Outcome**: Will the server win this rally? (Binary)

**Challenge**: Sequential prediction with temporal dependencies

### Approach

#### ‚úÖ Gold Baseline

**Features**:
- Rally phase (serve/return/early/extended)
- Action type grouping (attack/control/defensive/serve)
- Score differential
- Lag-1 and Lag-2 features (previous strokes)
- Tactical combinations: `prev_hand_spin`, `prev_action_phase`

**Model**: 50-50 LightGBM + CatBoost ensemble with 5-fold GroupKFold CV

---

#### üèÜ Gold + EDA: Distribution-Aware (Final Best)

**Key EDA Findings**:
- Test rallies 60% shorter (median 2 vs 5 strokes)
- Test has 2x more serves (32.7% vs 15.9%)
- Test favors forehand (61.0% vs 44.8%)

**Optimizations**:
- Early-phase focused features (`is_serve_phase`, `is_return_phase`)
- Distribution-aligned: `hand_spin`, `hand_strength` combinations
- Stronger regularization (lower lr, reduced complexity)
- Reduced lag-2 dependency (test rallies end earlier)

**Result**: +2.2 pts over Gold ‚Üí **0.3596 (Best)**

---

## üìà Performance Analysis

| Version | OOF (Action/Point/Outcome) | Public | Private | Delta |
|---------|----------------------------|--------|---------|-------|
| V6 | 0.35 / 0.35 / 0.65 | 0.3698 | 0.3419 | -0.0279 ‚ùå |
| Gold | 0.37 / 0.36 / 0.67 | 0.3205 | 0.3574 | +0.0369 ‚úÖ |
| Gold+EDA | 0.37 / 0.35 / 0.68 | 0.3189 | **0.3596** | +0.0407 üèÜ |

**Key Lesson**: Lower public score ‚â† worse model. Trust cross-validation!

---

## üîë Key Techniques

### 1. Prediction Synchronization
```python
# Ensures action/point predictions agree on rally endings
def synchronize_endings(prob_action, prob_point, threshold=0.5):
    # Average ending probabilities from both models
    p_end = (prob_action[:, -1] + prob_point[:, -1]) / 2
    
    # Boost when both agree (confidence high)
    if p_end >= threshold:
        prob_action[:, -1] = 2.0  # Amplify ending signal
        prob_point[:, -1] = 2.0
    else:
        prob_action[:, -1] = 0.0  # Suppress false endings
        prob_point[:, -1] = 0.0
    
    return argmax(prob_action), argmax(prob_point)
```

### 2. GroupKFold Cross-Validation
```python
# Prevents data leakage by keeping rallies intact
gkf = GroupKFold(n_splits=5)
for train_idx, val_idx in gkf.split(X, y, groups=rally_uid):
    # All strokes from same rally stay together
    # Validates true generalization to unseen rallies
```

### 3. Categorical Feature Handling
```python
# LightGBM: Native categorical support
lgb.Dataset(X, categorical_feature=['actionId', 'pointId', ...])

# CatBoost: Index-based categorical
Pool(X, cat_features=[0, 1, 2, ...])  # Column indices
```

---

## üìö Dependencies

```txt
# Core ML libraries
lightgbm==4.6.0
catboost==1.2.8
scikit-learn==1.6.1

# Data processing
pandas==2.3.3
numpy==2.3.5

# Visualization (for EDA)
matplotlib==3.10.7
seaborn==0.13.2

# Jupyter
ipykernel==7.1.0
jupyter==1.1.1
```

---

## üéì Lessons Learned

1. **Trust CV over public leaderboard**: Best public (V6) ‚â† best private
2. **Avoid information leakage**: Features must only use past information
3. **Simple beats complex**: Removing features improved +15.5 pts
4. **EDA is crucial**: Discovered distribution shifts post-competition
5. **Balance ensembles**: 50-50 blend > 80-20 for stability

---

## üîß Troubleshooting

### Issue: "FileNotFoundError: train.csv not found"
**Solution**: Ensure data files are in `data/` directory:
```bash
mkdir -p data
# Place train.csv, test.csv, sample_submission.csv in data/
```

### Issue: "GPU not available"
**Solution**: Set `USE_GPU = False` in first cell of notebook

### Issue: Training takes too long
**Solution**: Reduce iterations in notebooks:
```python
lgb_common['n_estimators'] = 1000  # Instead of 3000
cat_common['iterations'] = 800     # Instead of 2000
```

### Issue: Out of memory
**Solution**: Reduce n_folds or batch size:
```python
N_FOLDS = 3  # Instead of 5
```

---

## üìú License

This project is available for educational purposes.

---

## üôè Acknowledgments

- **Competition Organizers**: For the challenging competition
- **Kaggle Platform**: For hosting and data
- **AI Tools**: Gemini, ChatGPT, and Claude for assisting with code development and documentation
- **Open Source Community**: LightGBM, CatBoost, scikit-learn teams

---

**Last Updated**: 2025/11/18  
**Author**: yuunjiee  
**Status**: ‚úÖ Final Results Published
